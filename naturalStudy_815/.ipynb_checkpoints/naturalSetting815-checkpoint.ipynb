{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36e2aee-dfeb-4baa-84ec-cb22b74f1694",
   "metadata": {},
   "source": [
    "# Solution for Predicting Olympic Medal Count\n",
    "We have a historical dataset on the modern Olympic Games, including all the Games from Athens 1896 to Rio 2012. We want to predict the number of medals for the 2016 Olympic Games. The dataset can be divided in three tables, the ground truth, country of olympics and the specific event with medals.\\\n",
    "Here are the attributes of our tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decddabc-7c52-42c3-9114-f1ac7b5574b2",
   "metadata": {},
   "source": [
    "<p><c>\n",
    "    <img src=\"dataset.png\" alt=\"Olympic Logo\" width=500/>\n",
    "</c></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2523aa98-2da1-43a1-82b8-0a3b7c4453c9",
   "metadata": {},
   "source": [
    "Here we provide a solution for predicting Olympic Medal Count in 2016. Now you need to **follow the provided solution and select a subset of features** you think is helpful for predicting the Medal Count of the coming Olympic Games  in Tokyo. You are required to finish the tasks below:\n",
    "### Steps:\n",
    "\n",
    "#### 1. learn about the dataset and the task of predicting Olympic Medal Count in the first cell called load data (input, ground truth, data, regressive model);\n",
    "#### 2. load the suggested features from experts and Automatic algorithms and <span style=\"color:blue\">check the features in the 3rd cell by clicking the button on the right of the cell</span>.\n",
    "#### 3. Checked the information of the features in the table view and select a subset of features by ticking the checkbox, submit your selection by clicking the <span style=\"color:blue\">blue button Submit Selection in the view</span>.\n",
    "#### 4. Copy the feature ids and evaluate these features in the evaluation cell by pasting the id list in the <span style=\"color:blue\">auto_human_feature_matrix.iloc[:,[feature id list]</span>.\n",
    "\n",
    "#### You can select and evaluate the features iteratively or create new features from the three tables by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7926275-0806-41dc-96dc-7d740131a9f8",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ee3be622-719b-4c91-a9a6-05d883f9992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and groung truth\n",
    "import os\n",
    "import pandas as pd\n",
    "import utils_original as utils\n",
    "import foldCode1 as load_df\n",
    "import foldCode2 as our_model\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(),\"data/olympic_games_data\")\n",
    "es = utils.load_entityset(data_dir=DATA_DIR)\n",
    "groundtruth_table, countries_table, events_table, cutoff_times_gt, dates, labels = load_df.loadData(es, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad40b99-8bc7-4de4-a0e5-315d7a313e10",
   "metadata": {},
   "source": [
    "You can add new cells to print the 3 tables (groundtruth_table, countries_table, events_table) and the input (dates) and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919bb20-cb6f-44a0-b32e-6c0b82c535c6",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\">Generate Features</span> (Edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc137103-136d-4199-91c6-64876537fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_human_feature_names, select_human_features = load_df.human_features() #7\n",
    "# all_features = pd.read_csv(\"features/natural_features_all_815.csv\")\n",
    "test = pd.read_csv(\"../source/natural_814.csv\", error_bad_lines=False)\n",
    "# check feature matrix\n",
    "# all_features\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c730c7d5-0c7e-44ac-ba17-fe0ee5508875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "490abbca-3815-4ed3-b682-bcdfb20af993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: write new features\n",
    "# all_features.insert()\n",
    "\n",
    "# OPTIONAL: you can check single feature:\n",
    "# all_features.iloc[:, #id]\n",
    "\n",
    "# MUST EDIT1: feature selection: input feature ID in the list below:\n",
    "# feature_id = [] #loc 0-98"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aabd46-271c-4932-b2dc-83ba18719aee",
   "metadata": {},
   "source": [
    "## Evaluate features with this given models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3ecb9649-6b61-416b-be80-40be4ac06d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         r2        mse Olympics Year\n",
      "0  0.812786  62.095205    2016-08-05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import utils_original as utils\n",
    "\n",
    "# parameters of the regressive model\n",
    "pipeline_preprocessing = [(\"imputer\",\n",
    "                            SimpleImputer()),\n",
    "                            (\"scaler\", RobustScaler(with_centering=True))]\n",
    "splitter = utils.TimeSeriesSplitByDate(dates=dates, earliest_date=pd.Timestamp('01/01/2012')) # using data before and contain 2012 for training\n",
    "\n",
    "# default all features\n",
    "all_X = all_features.values  \n",
    "\n",
    "# MUST EDIT2: selection input, you can edit here to evalute feature subsets\n",
    "# all_X = all_features.iloc[:, feature_id].values\n",
    "\n",
    "rf_regressor = RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        random_state=50\n",
    "    )\n",
    "pipeline_reg = Pipeline(pipeline_preprocessing + [('rf_reg', rf_regressor)])\n",
    "regression_score = utils.fit_and_score(all_X, labels, splitter, pipeline_reg)\n",
    "\n",
    "print(regression_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f84b392-ae34-4014-bfc0-7a71f69f49d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the importance score of selected features\n",
    "feature_imp = utils.get_feature_importances(pipeline_reg, \n",
    "                                             all_features,\n",
    "                                            # MUST EDIT3: \n",
    "#                                             all_features.iloc[:, feature_id], # edit here to check the score of selected features\n",
    "                                            labels, splitter, 300)\n",
    " \n",
    "test_date = pd.Timestamp('08/05/2016')\n",
    "feature_imp[test_date].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ba211-0d75-47b0-9f9b-38ec78e975ea",
   "metadata": {},
   "source": [
    "### Optional: Try multiple regressive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "361b9ed1-795d-461d-9814-c557310c4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import ARDRegression\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "logistic_reg = LogisticRegression()\n",
    "sgd_reg = SGDRegressor()\n",
    "knn_reg = KNeighborsRegressor()\n",
    "mlp_reg = MLPRegressor()\n",
    "decisionTree_reg = DecisionTreeRegressor()\n",
    "extTree_reg = ExtraTreeRegressor()\n",
    "gb_reg = GradientBoostingRegressor()\n",
    "svc_reg = LinearSVC()\n",
    "svr_reg = LinearSVR()\n",
    "ard_reg = ARDRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "047df309-a74d-4617-92df-0e2b01aa353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_reg: \n",
      "         r2        mse Olympics Year\n",
      "0  0.776419  74.157535    2016-08-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuqian/conda3/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_reg: \n",
      "         r2         mse Olympics Year\n",
      "0  0.163387  277.488372    2016-08-05\n",
      "sgd_reg: \n",
      "             r2           mse Olympics Year\n",
      "0 -1.912959e+25  6.344913e+27    2016-08-05\n",
      "knn_reg: \n",
      "         r2        mse Olympics Year\n",
      "0  0.715974  94.206047    2016-08-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuqian/conda3/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp_reg: \n",
      "         r2        mse Olympics Year\n",
      "0  0.720006  92.868699    2016-08-05\n",
      "decisionTree_reg: \n",
      "         r2        mse Olympics Year\n",
      "0  0.774229  74.883721    2016-08-05\n",
      "extTree_reg: \n",
      "         r2        mse Olympics Year\n",
      "0  0.729706  89.651163    2016-08-05\n",
      "gb_reg: \n",
      "        r2        mse Olympics Year\n",
      "0  0.80146  65.851717    2016-08-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuqian/conda3/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/zhuqian/conda3/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc_reg: \n",
      "         r2         mse Olympics Year\n",
      "0  0.513997  161.197674    2016-08-05\n",
      "svr_reg: \n",
      "         r2        mse Olympics Year\n",
      "0  0.771575  75.764046    2016-08-05\n",
      "ard_reg: \n",
      "         r2        mse Olympics Year\n",
      "0  0.755657  81.043935    2016-08-05\n"
     ]
    }
   ],
   "source": [
    "pipeline_reg1 = Pipeline(pipeline_preprocessing + [('linear_reg', linear_reg)])\n",
    "regression_score1 = utils.fit_and_score(all_X, labels, splitter, pipeline_reg1)\n",
    "print(\"linear_reg: \")\n",
    "print(regression_score1)\n",
    "\n",
    "pipeline_reg2 = Pipeline(pipeline_preprocessing + [('logistic_reg', logistic_reg)])\n",
    "regression_score2 = utils.fit_and_score(all_X, labels, splitter, pipeline_reg2)\n",
    "print(\"logistic_reg: \")\n",
    "print(regression_score2)\n",
    "\n",
    "pipeline_reg3 = Pipeline(pipeline_preprocessing + [('sgd_reg', sgd_reg)])\n",
    "regression_score3 = utils.fit_and_score(all_X, labels, splitter, pipeline_reg3)\n",
    "print(\"sgd_reg: \") \n",
    "print(regression_score3)\n",
    "\n",
    "pipeline_reg4 = Pipeline(pipeline_preprocessing + [('knn_reg', knn_reg)])\n",
    "regression_score4 = utils.fit_and_score(all_X, labels, splitter, pipeline_reg4)\n",
    "print(\"knn_reg: \")\n",
    "print(regression_score4)\n",
    "\n",
    "pipeline_reg5 = Pipeline(pipeline_preprocessing + [('mlp_reg', mlp_reg)])\n",
    "regression_score5 = utils.fit_and_score(all_X, labels, splitter, pipeline_reg5)\n",
    "print(\"mlp_reg: \")\n",
    "print(regression_score5)\n",
    "\n",
    "pipeline_reg6 = Pipeline(pipeline_preprocessing + [('decisionTree_reg', decisionTree_reg)])\n",
    "regression_score6 = utils.fit_and_score(all_X, labels, splitter, pipeline_reg6)\n",
    "print(\"decisionTree_reg: \")\n",
    "print(regression_score6)\n",
    "\n",
    "pipeline_reg7 = Pipeline(pipeline_preprocessing + [('extTree_reg', extTree_reg)])\n",
    "regression_score7 = utils.fit_and_score(all_X, labels, splitter, pipeline_reg7)\n",
    "print(\"extTree_reg: \")\n",
    "print(regression_score7)\n",
    "\n",
    "pipeline_reg8 = Pipeline(pipeline_preprocessing + [('gb_reg', gb_reg)])\n",
    "regression_score8 = utils.fit_and_score(all_X, labels, splitter, pipeline_reg8)\n",
    "print(\"gb_reg: \")\n",
    "print(regression_score8)\n",
    "\n",
    "pipeline_reg9 = Pipeline(pipeline_preprocessing + [('svc_reg', svc_reg)])\n",
    "regression_score9 = utils.fit_and_score(all_X, labels, splitter, pipeline_reg9)\n",
    "print(\"svc_reg: \")\n",
    "print(regression_score9)\n",
    "\n",
    "pipeline_reg10 = Pipeline(pipeline_preprocessing + [('svr_reg', svr_reg)])\n",
    "regression_score10 = utils.fit_and_score(all_X, labels, splitter, pipeline_reg10)\n",
    "print(\"svr_reg: \")\n",
    "print(regression_score10)\n",
    "\n",
    "pipeline_reg11 = Pipeline(pipeline_preprocessing + [('ard_reg', ard_reg)])\n",
    "regression_score11 = utils.fit_and_score(all_X, labels, splitter, pipeline_reg11)\n",
    "print(\"ard_reg: \")\n",
    "print(regression_score11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0fa706-8155-494b-a6f0-c396438ceffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the importance score of selected features\n",
    "feature_imp = utils.get_feature_importances(pipeline_reg8, # EDIT: model name\n",
    "                                             all_features,\n",
    "#                                             all_features.iloc[:, feature_id], # edit here to check the score of selected features\n",
    "                                            labels, splitter, 300)\n",
    " \n",
    "test_date = pd.Timestamp('08/05/2016')\n",
    "feature_imp[test_date].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e998716-c465-42af-86f0-c1db943a03fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
